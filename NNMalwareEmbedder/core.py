''' NNMalwareEmbedder.core contains basic functionality of Embedders.

classes:
    1. Payload - class to store payload
    2. BaseEmbedder - abstract base embedder
    3. LsbEmbedder - embedder implementing lsb subtitution method
    4. ResilienceTrainEmbedder - embedder implementing resilience training method
    5. ValueMappingEmbedder - embedder implementing value-mapping method
    6. SignMappingEmbedder - embedder implementing sign-mapping method 
    7. MsbEmbedder - embedder implementing msb subtitution method
    8. HalfEmbedder - embedder implementing half subtitution method
    9. FastEmbedder - embedder implementing fast subtitution method
'''

import torch
import copy

from os.path import realpath
from functools import reduce

from NNMalwareEmbedder.util import Encoder


class Payload:
    ''' Payload class to store payload.
    
    atributes:
        1. file_path: string - absolute or relative path to file with payload
        2. binary: string - binary representation of payload
        3. hash: int - hash value of binary payload
    '''
    
    def __init__(self, file_path=''):
        self._hash = None
        self._binary = None
        self.file_path = file_path
    
    @property
    def binary(self):
        return self._binary
    
    @binary.setter
    def binary(self, binary_value):
        self._binary = binary_value
        self._hash = hash(self.hash)
    
    @property
    def file_path(self):
        return self._file_path
    
    @file_path.setter
    def file_path(self, file_path):
        if file_path != '':
            self._file_path = file_path
            self.binary = Encoder.encode_file(file_path, 'rb')
    
    @property
    def hash(self):
        return self._hash
        
    def get_bits(self, bit_num):
        ''' Generate next portion of binary payload.
        
        Args:
            bit_num: int - size of portion to be returned
        '''
        for i in range(0, len(self.binary), bit_num):
            yield self.payload.binary[i:
                min(i+bit_num, len(self.payload.binary))]
    


class BaseEmbedder:
    ''' BaseEmbedder - abstract class with basic functionality for embedders.
    
    atributes:
        1. model: torch.nn.Module - model in which malware will be embedded
        2. layers: list[int] - list of layers in which malware will be embedded
    +   3. payload: Payload - payload to be embedded
    -   3. payload_path: string - absolute or relative path to malware file
    -   4. payload_binary: string - binary representation of malware
        5. index_permutation: list[tuple[int]] - list of parameters' coordinates to embed malware
        6. shadow_model: torch.nn.Module - clone of the model if embedding in not inplace
    
    methods:
    -   1. set_payload_path - set path to payload file
    -   2. set_payload_binary - set binary string as payload
        3. set_index_permutation - set coordinates of parameters to embed malware
        4. reset_index_permutation - index_pemutation field = None
    -   5. get_payload_hash - get hash value of binary payload
    -   6. get_extracted_payload_hash - get hash value of binary 
        7. get_num_of_params - get number of parameters to embed malware
        8. check_embeddability - raises exceptions if malware is not embeddable else return True
        9. set_strategy - set strategy to embed malware
       10. set_strategy_index_permutation - set index_permutation by strategy
    -  11. payload_bits - generator function to get payload's next binary portion 
       12. updated_param - return float value with embedded payload 
       13. _embed_payload - embeds payload to model or model's copy
       14. embed_payload - checks for embeddability and runs _embed_paylaod
       15. get_model_info - return list of embedding printable info blocks
       16. print_model_info - print embedding info blocks
    '''
    
    def __init__(self, model, payload_path='', layers=None):
        ''' __init__ function for BaseEmbedder
        
        Args:
            model: torch.nn.Module - model in which malware will be embedded
            layers: list[int] - list of layers in which malware will be embedded
        '''
        self.model = model
        self.layers = layers
        
        self.payload = Payload(payload_bath)
        
        self.index_permutation = None
        self.shadow_model = None
    
    def set_index_permutation(self, index_permutation=None):
        self.index_permutation = index_permutation
    
    def reset_index_permutation(self):
        self.index_permutation = None
    
    def get_num_of_params(self):
        raise NotImplementedError(
            'Function get_num_of_params is not implemented to BaseEmbedder. ' + 
            'Consider using one of the subclasses')
        
    def check_embeddability(self):
        assert self.payload_bytes is not None, \
            'Payload is absent'
        necessary_params = self.get_num_of_params()
        total_params = 0
        for i, (name, layer) in enumerate(self.model.named_parameters()):
            if i in self.layers:
                total_params += reduce(lambda a, b: a*b, list(layer.shape))
        assert total_params >= necessary_params, \
            'Too few parameters are chosen or in the model'
    
        #for i, layer in enumerate(self.model.children()):
        #    if i in self.layers:
        #        assert hasattr(layer, 'weight'), \
        #            'One of the chosen layers has no weights'
        
        if self.index_permutation is not None:
            assert len(set(self.index_permutation)) == len(self.index_permutation), \
                'Looks like index permutation contains duplicate values'
            
            # Check that all layers in index permutation are layers with weights
            #index_layers = set(map(lambda coord: coord[0], self.index_permutation))
            #layers_without_weights = []
            #for i, layer in enumerate(self.model.children()):
            #    if not hasattr(layer, 'weight'):
            #        layers_without_weights.append(i)
            #assert True not in map(lambda a: a in layers_without_weights, index_layers), \
            #    'One of layers in index permutation refers to layer without weights'
        
        return True
    
    def set_strategy(self, strategy):
        # 'first', 'manual'
        self.strategy = strategy
    
    def set_strategy_index_permutation(self):
        raise NotImplementedError(
            'Function get_strategy_weights is not implemented to BaseEmbedder. ' + 
            'Consider using one of the subclasses')

    def updated_param(self, param, new_bits):
        raise NotImplementedError(
            'Function update_param is not implemented to BaseEmbedder. ' + 
            'Consider using one of the subclasses')
    
    # def _embed_method_info(self, inplace=False):
    #     raise NotImplementedError(
    #         'Function _embed_method_info is not implemented to BaseEmbedder. ' + 
    #         'Consider using one of the subclasses')
    
    def _embed_payload(self, inplace=False):
        
        self.check_embeddability()
        
        # INPLACE
        if inplace:
            model = self.model
        else:
            model = self.shadow_model
            
        # EMBEDDING
        with torch.no_grad():
        
            next_bits = self.payload_bits()
        
            for index in self.index_permutation:
                
                index_layer = index[0]
                index_coord = tuple(index[1:])
            
                for i, (name, layer) in enumerate(model.named_parameters()):
                
                    if i == index_layer:
                        new_bits = next(next_bits)
                        new_value = self.updated_param(
                                    layer[index_coord], new_bits)
                        layer[index_coord] = new_value
    
    
    def embed_payload(self, inplace=False):
        self.check_embeddability()
        if not inplace:
            self.shadow_model = copy.deepcopy(self.model)
        self._embed_payload(inplace)
    
    
    def get_model_info(self):
        title_str_begin = "======================= EMBEDDING INIT SUMMARY ========================="

        layer_str = "Layer Info:\n"
        layer_str += (
            "------------------------------------------------------------------------"
            + "\n"
        )
        line_new = "{:>5}  {:>15}  {:>10} {:>20} {:>13}".format(
            "Layer #", "Layer type", "Dimensions", "Weight Shape", "Params #"
        )
        layer_str += line_new + "\n"
        layer_str += (
            "------------------------------------------------------------------------"
            + "\n"
        )
        total_params = 0
        for index, (name, layer) in enumerate(self.model.named_parameters()):

            params = reduce(lambda a, b: a*b, list(layer.shape))
            total_params += params
            dimensions = str(len(layer.shape))
            shape = str(list(layer.shape))
            
            line_new = "{:>5}  {:>15}  {:>10} {:>20} {:>13}".format(
                str(index),
                name,
                str(dimensions),
                str(shape),
                str(params),
            )
            layer_str += line_new + "\n"

        title_str_end = (
            "========================================================================"
        )

        params_str = "Total params: {}\n".format(total_params)
        
        return [title_str_begin,
            layer_str,
            title_str_end,
            params_str]
    
    def print_model_info(self):
        print('\n'.join(self.get_model_info()))




class LsbEmbedder(BaseEmbedder):
    
    def __init__(self, model, bit_length, layers=None, param_length=32):
        super().__init__(model, layers)
        self.param_length = param_length
        self.bit_length = bit_length
    
    def get_num_of_params(self):
        num_of_params = (len(self.payload_bytes) + 1) // self.bit_length
        return num_of_params
    
    
    def check_embeddability(self):
        assert self.bit_length is not None, \
            'You must specify lsb bit_length'
    
        super().check_embeddability()
        
        necessary_params = self.get_num_of_params()
        if self.index_permutation is not None:
            assert len(self.index_permutation) * self.bit_length >= necessary_params, \
                'Too few parameters are in index permutation to fit the payload'
        
        return True
    
    def set_strategy_index_permutation(self):
        
        self.check_embeddability()
        
        def inc(coord, shape):
            if coord[-1] + 1 >= shape[-1]:
                if len(coord) == 1:
                    return [0] * len(shape)
                return inc(coord[:-1], shape[:-1]) + [0]
            return coord[:-1] + [coord[-1] + 1]
        
        def is_zero(coord):
            for i in coord:
                if i != 0:
                    return False
            return True
    
        param_num = self.get_num_of_params()
        if self.strategy == 'first':
            self.index_permutation = []
            with torch.no_grad():
                for i, (name, layer) in enumerate(self.model.named_parameters()):
                    if i in self.layers:
                        coord = [0] * len(layer.shape)
                        while param_num > 0:
                            self.index_permutation.append((i, *coord))
                            coord = inc(coord, layer.shape)
                            param_num -= 1
                            if is_zero(coord):
                                break
                        if param_num <= 0:
                            break
        
        self.check_embeddability()
            
    
    def updated_param(self, param, new_bits):
        bit_length = len(new_bits)
        bin_param = Encoder.encode_float(param, self.param_length)
        bin_param = bin_param[:-bit_length] + new_bits
        return Encoder.decode_float(bin_param, self.param_length)
    
    
    def extract(self):
        #self.bit_length
        #self.payload_hash     # TODO
        self.last_bit_length = self.bit_length
        #self.index_permutation
        
        if self.shadow_model is not None:
            model = self.shadow_model
        else:
            model = self.model
        
        payload = ''
        for coord_i, coord in enumerate(self.index_permutation):
            coord_layer = coord[0]
            coord_param = tuple(coord[1:])
            for i, (name, layer) in enumerate(model.named_parameters()):
                if coord_layer == i:
                    bin_val = Encoder.encode_float(layer[coord_param], self.param_length)
                    if coord_i == len(self.index_permutation)-1:
                        payload_bin = bin_val[-self.last_bit_length:]
                    else:
                        payload_bin = bin_val[-self.bit_length:]
                    payload = payload + payload_bin
        self.ext_payload_bytes = payload
        return payload
    
    
    def print_model_info(self):
        [title_str_begin,
        layer_str,
        title_str_end,
        params_str] = self.get_model_info()
        
        method_str = "Method Info:\n"
        method_str += "------------------------------------------------------------------------"
        method_str += "\n"
        method_str += "    - Method: LSB\n"
        method_str += "    - Strategy: first\n"
        method_str += "    - LSB bit length: " + str(self.bit_length)
        method_str += "\n"
        
        if self.layers is not None:
            method_str += "    - Embedding Layers: " + str(self.layers) + "\n"
        
        print(title_str_begin)
        print(method_str)
        print(layer_str)
        print(title_str_end)
        print(params_str)











