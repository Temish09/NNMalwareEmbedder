''' NNMalwareEmbedder.embedder contains basic functionality of Embedders.

classes:
    1. BaseEmbedder - abstract base embedder;
    2. LsbEmbedder - embedder implementing lsb subtitution method;
    3. ResilienceTrainEmbedder - embedder implementing resilience training method;
    4. ValueMappingEmbedder - embedder implementing value-mapping method;
    5. SignMappingEmbedder - embedder implementing sign-mapping method;
    6. MsbEmbedder - embedder implementing msb subtitution method;
    7. HalfEmbedder - embedder implementing half subtitution method;
    8. FastEmbedder - embedder implementing fast subtitution method;
'''

import copy
import warnings
from functools import reduce
from math import ceil

import torch

from .core import Payload
from .util import Encoder

class BaseEmbedder:
    ''' Abstract class with basic functionality for embedders.

    atributes:
        1. model: torch.nn.Module - model in which malware will be embedded;
        2. layers: list[int] - list of layers in which malware will be embedded;
        3. param_length: int - bit length of parameters. By default float is 32 bits.
        4. payload: Payload - payload to be embedded;
        5. index_permutation: list[tuple[int]] - list of parameters' coordinates to embed malware;
        6. shadow_model: torch.nn.Module - clone of the model if embedding in not inplace;

    methods:
        1. get_num_of_params - get number of parameters to embed malware;
        2. check_embeddability - raises exceptions if malware is not embeddable else return True;
        3. set_strategy - set strategy to embed malware;
        4. set_strategy_index_permutation - automaticaly set index_permutation by chosen strategy;
        5. updated_param - return float value with embedded payload;
        6. _embed_payload - embeds payload to model or model's copy;
        7. embed_payload - checks for embeddability and runs _embed_payload;
        8. get_model_info - return list of embedding printable info blocks;
        9. print_model_info - print embedding info blocks;
    '''
    def __init__(self, model, payload_path='', param_length=32):
        ''' __init__ function for BaseEmbedder.

        Args:
            model: torch.nn.Module - model in which malware will be embedded
            payload_path: str - path to malware file
        '''
        self.model = model
        self.payload = Payload(payload_path)

        self.layers = []
        self.param_length = param_length

        self.strategy = None
        self.index_permutation = None
        self.shadow_model = None

    def get_num_of_params(self):
        ''' Get necessary number of parameters to embed payload.

        Returns:
            int - number of parameters to be updated.
        '''
        raise NotImplementedError(
            'Function get_num_of_params is not implemented to BaseEmbedder. ' +
            'Consider using one of the subclasses')

    def check_embeddability(self):
        ''' Check if payload is embeddable to model with chosen layers and strategy.

        Raises:
            AssertionError - if payload is not embeddable

        Returns:
            True - if payload is embeddable
        '''
        assert self.payload.binary is not None, \
            'Payload is absent'

        if self.layers == []:
            warnings.warn("Layers is empty. Algorithms will use all layers by default",
                          warnings.UserWarning)

        necessary_params = self.get_num_of_params()
        total_params = 0
        for i, (_, layer) in enumerate(self.model.named_parameters()):
            if (i in self.layers) or (self.layers == []):
                total_params += reduce(lambda a, b: a*b, list(layer.shape))
        assert total_params >= necessary_params, \
            'Too few parameters are chosen or in the model'

        if self.index_permutation is not None:
            assert len(set(self.index_permutation)) == len(self.index_permutation), \
                'Looks like index permutation contains duplicate values'

        if self.index_permutation is not None:
            for index in self.index_permutation:
                layer = index[0]
                coord = index[1:]
                for i, (_, param) in enumerate(self.model.named_parameters()):
                    if layer == i:
                        assert all(x < y for x, y in zip(coord, tuple(param.data.shape))), \
                            'Coord {} in index_permutation is out of bounds'.format(index)
                        break
        return True

    def set_strategy(self, strategy):
        ''' Set current strategy for embedding payload into model.

        Raises:
            AssertionError - if strategy is not in list of standart strategies.

        Args:
            strategy: str - strategy name.
        '''
        strats = ('manual', 'first', 'uniform', 'neuron', 'random', 'distributed', 'least')
        assert strategy in strats, \
            'Strategy {} is not in list of standart strategies'.format(strategy)
        self.strategy = strategy

    def set_strategy_index_permutation(self):
        ''' Set index permutation with chosen strategy. '''
        self.check_embeddability()
        if self.strategy == 'first':
            self._strategy_first()
        elif self.strategy == 'uniform':
            self._strategy_uniform()
        elif self.strategy == 'neuron':
            self._strategy_neuron()
        elif self.strategy == 'random':
            self._strategy_random()
        elif self.strategy == 'distributed':
            self._strategy_distributed()
        elif self.strategy == 'least':
            self._strategy_least()

    def _strategy_first(self):
        ''' Set index permutation by strategy 'first'.

        Strategy:
            In chosen layers all parameters will be updated subsequently
            while payload is not embeded entirely.
        '''
        def inc(coord, shape):
            if coord[-1] + 1 >= shape[-1]:
                if len(coord) == 1:
                    return [0] * len(shape)
                return inc(coord[:-1], shape[:-1]) + [0]
            return coord[:-1] + [coord[-1] + 1]

        param_num = self.get_num_of_params()
        self.index_permutation = []
        with torch.no_grad():
            for i, (_, layer) in enumerate(self.model.named_parameters()):
                if i in self.layers:
                    coord = [0] * len(layer.shape)
                    while param_num > 0:
                        self.index_permutation.append((i, *coord))
                        coord = inc(coord, layer.shape)
                        param_num -= 1

                        if all(map(lambda x: x == 0, coord)):
                            break

                    if param_num <= 0:
                        break

    def _strategy_uniform(self):
        ''' Set index permutation by strategy 'uniform'.

        Strategy:
            In chosen layers updated parameters will be distributed uniformly.
        '''
        self.index_permutation = []

    def _strategy_neuron(self):
        pass

    def _strategy_random(self):
        pass

    def _strategy_distributed(self):
        pass

    def _strategy_least(self):
        pass

    def updated_param(self, param, new_bits):
        ''' Embed payload bits to parameter.

        Args:
            param: float - parameter in which payload bits will be embeded.
            new_bits: string - binary payload bits.

        Returns:
            float: new updated parameter.
        '''
        raise NotImplementedError(
            'Function update_param is not implemented to BaseEmbedder. ' +
            'Consider using one of the subclasses')

    def _embed_payload(self, inplace=False):
        ''' Update parameters according chosen layers, strategy and index permutation.

        Args:
            inplace: bool - change original model or model's copy
                True: original model will be updated;
                False: model's copy 'shadow_model' will be updated.
        '''
        # INPLACE
        if inplace:
            model = self.model
        else:
            model = self.shadow_model

        # EMBEDDING
        with torch.no_grad():
            
            # WTF
            bits = self.payload.get_bits(self.bit_length)

            for index in self.index_permutation:

                index_layer = index[0]
                index_coord = tuple(index[1:])

                for i, (_, layer) in enumerate(model.named_parameters()):

                    if i == index_layer:
                        next_bits = next(bits)
                        new_value = self.updated_param(
                            layer[index_coord], next_bits)
                        layer[index_coord] = new_value
                        break


    def embed_payload(self, inplace=False):
        ''' Update parameters according chosen layers, strategy and index permutation.

        Args:
            inplace: bool - change original model or model's copy
                True: original model will be updated;
                False: model's copy 'shadow_model' will be updated.
        '''
        self.check_embeddability()
        if not inplace:
            self.shadow_model = copy.deepcopy(self.model)
        self._embed_payload(inplace)


    def get_model_info(self):
        ''' Return the list of printable inforamtion about model and embedding method."

        Returns:
            list - list of informational str's.
        '''
        title_str_begin = "======================= EMBEDDING INIT SUMMARY ========================="

        layer_str = "Layer Info:\n"
        layer_str += (
            "------------------------------------------------------------------------"
            + "\n"
        )
        line_new = "{:>7}  {:>20}  {:>5} {:>20} {:>13}".format(
            "Layer #", "Layer type", "Dims", "Weight Shape", "Params #"
        )
        layer_str += line_new + "\n"
        layer_str += (
            "------------------------------------------------------------------------"
            + "\n"
        )
        total_params = 0
        for index, (name, layer) in enumerate(self.model.named_parameters()):

            params = reduce(lambda a, b: a*b, list(layer.shape))
            total_params += params
            dimensions = str(len(layer.shape))
            shape = str(list(layer.shape))

            line_new = "{:>7}  {:>20}  {:>5} {:>20} {:>13}".format(
                str(index),
                name,
                str(dimensions),
                str(shape),
                str(params),
            )
            layer_str += line_new + "\n"

        title_str_end = (
            "========================================================================"
        )

        params_str = "Total params: {}\n".format(total_params)

        return [title_str_begin,
                layer_str,
                title_str_end,
                params_str]

    def print_model_info(self):
        '''Print information about embedding method and model.'''
        print('\n'.join(self.get_model_info()))




class LsbEmbedder(BaseEmbedder):
    ''' Embedder implementing LSB substitution method.

    atributes:
        1. model: torch.nn.Module - model in which malware will be embedded;
        2. layers: list[int] - list of layers in which malware will be embedded;
        3. param_length: int - bit length of parameters. By default float is 32 bits.
        4. payload: Payload - payload to be embedded;
        5. index_permutation: list[tuple[int]] - list of parameters' coordinates to embed malware;
        6. shadow_model: torch.nn.Module - clone of the model if embedding in not inplace;
        7. bit_length: int - least significant bits length;
        8. last_bits_length: int - if payload's length is not divisible by ``bit_length``
                last_bits_length stores length of remainder bits.
        9. ext_payload: Payload - extracted payload from the model.

    methods:
        1. get_num_of_params - get number of parameters to embed malware;
        2. check_embeddability - raises exceptions if malware is not embeddable else return True;
        3. set_strategy - set strategy to embed malware;
        4. set_strategy_index_permutation - automaticaly set index_permutation by chosen strategy;
        5. updated_param - return float value with embedded payload;
        6. _embed_payload - embeds payload to model or model's copy;
        7. embed_payload - checks for embeddability and runs _embed_payload;
        8. get_model_info - return list of embedding printable info blocks;
        9. print_model_info - print embedding info blocks;
       10. extract - extracts payload and writes extracted payload to ``ext_payload`` variable.
    '''
    def __init__(self, model, bit_length, payload_path='', param_length=32):
        ''' Initialize LsbEmbedder.

        Args:
            model: torch.nn.Module - model to embed payload.
            bit_length: int - number of least significant bits.
            payload_path: str - path to a file with payload.
            param_length: int - length of model's parameters. Default float is 32 bit length.
        '''
        super().__init__(model, payload_path, param_length)
        self.bit_length = bit_length

        self.last_bits_length = 0
        self.ext_payload = None

    def get_num_of_params(self):
        ''' Get necessary number of parameters to embed payload.

        Returns:
            int - number of parameters to be updated.
        '''
        num_of_params = ceil(len(self.payload.binary) / self.bit_length)
        return num_of_params


    def check_embeddability(self):
        ''' Check if payload is embeddable to model with chosen layers and strategy.

        Raises:
            AssertionError - if payload is not embeddable

        Returns:
            True - if payload is embeddable
        '''
        assert self.bit_length is not None, \
            'You must specify lsb bit_length'

        super().check_embeddability()

        necessary_params = self.get_num_of_params()
        if self.index_permutation is not None:
            assert len(self.index_permutation) * self.bit_length >= necessary_params, \
                'Too few parameters are in index permutation to fit the payload'

        return True


    def updated_param(self, param, new_bits):
        ''' Embed payload bits ``new_bits`` to parameter ``param``.

        Args:
            param: float - parameter in which payload bits will be embeded.
            new_bits: string - binary payload bits.

        Returns:
            float: new updated parameter.
        '''
        bit_length = len(new_bits)
        bin_param = Encoder.encode_float(param, self.param_length)
        bin_param = bin_param[:-bit_length] + new_bits
        return Encoder.decode_float(bin_param, self.param_length)


    def extract(self):
        ''' Extract payload from the model and save it as ``ext_payload``.

        Returns:
            Payload - extracted payload.
        '''

        if self.shadow_model is not None:
            model = self.shadow_model
        else:
            model = self.model

        payload = ''
        for coord_i, coord in enumerate(self.index_permutation):
            coord_layer = coord[0]
            coord_param = tuple(coord[1:])
            for i, (_, layer) in enumerate(model.named_parameters()):
                if coord_layer == i:
                    bin_val = Encoder.encode_float(layer[coord_param], self.param_length)
                    if coord_i == len(self.index_permutation)-1:
                        payload_bin = bin_val[-self.last_bits_length:]
                    else:
                        payload_bin = bin_val[-self.bit_length:]
                    payload = payload + payload_bin
        self.ext_payload = Payload(binary=payload)
        return self.ext_payload


    def print_model_info(self):
        '''Print information about embedding method and model.'''
        [title_str_begin,
         layer_str,
         title_str_end,
         params_str] = self.get_model_info()

        method_str = "Method Info:\n"
        method_str += "------------------------------------------------------------------------"
        method_str += "\n"
        method_str += "    - Method: LSB\n"
        method_str += "    - Strategy: first\n"
        method_str += "    - LSB bit length: " + str(self.bit_length)
        method_str += "\n"

        if self.layers is not None:
            method_str += "    - Embedding Layers: " + str(self.layers) + "\n"

        print(title_str_begin)
        print(method_str)
        print(layer_str)
        print(title_str_end)
        print(params_str)