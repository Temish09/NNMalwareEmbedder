import torch
import copy

from os.path import realpath
from functools import reduce

from NNMalwareEmbedder.util import Encoder


class BaseEmbedder:
    
    def __init__(self, model, layers=None):
        self.model = model
        self.layers = layers
        
        self.payload_path = ''
        self.payload_text = None
        self.payload_bytes = None
        
        self.index_permutation = None
        self.shadow_model = None

    def set_payload_path(self, payload_path):
        if realpath(self.payload_path) == realpath(payload_path):
            return
        self.payload_path = payload_path
        self.payload_bytes = Encoder.encode_file(payload_path)
        self.payload_text = Encoder.decode_string(self.payload_bytes)
        self.payload_hash = hash(self.payload_bytes)
    
    def set_payload_bytes(self, payload_bytes):
        if self.payload_bytes ==  payload_bytes:
            return
        self.payload_path = ''
        self.payload_bytes = payload_bytes
        self.payload_text = Encoder.decode_string(payload_bytes)
        self.payload_hash = hash(self.payload_bytes)
    
    def set_payload_text(self, payload_text):
        if self.payload_text ==  payload_text:
            return
        self.payload_path = ''
        self.payload_bytes = Encoder.encode_string(payload_text)
        self.payload_text = payload_text
        self.payload_hash = hash(self.payload_bytes)
    
    def set_index_permutation(self, index_permutation=None):
        self.index_permutation = index_permutation
    
    def reset_index_permutation(self):
        self.index_permutation = None
    
    def get_payload_hash(self):
        if self.payload_bytes is None:
            return None
        return hash(self.payload_bytes)
    
    def get_extracted_payload_hash(self):
        if self.ext_payload_bytes is None:
            return None
        return hash(self.ext_payload_bytes)
    
    def get_num_of_params(self):
        raise NotImplementedError(
            'Function get_num_of_params is not implemented to BaseEmbedder. ' + 
            'Consider using one of the subclasses')
        
    def check_embeddability(self):
        assert self.payload_bytes is not None, \
            'Payload is absent'
        necessary_params = self.get_num_of_params()
        total_params = 0
        for i, (name, layer) in enumerate(self.model.named_parameters()):
            if i in self.layers:
                total_params += reduce(lambda a, b: a*b, list(layer.shape))
        assert total_params >= necessary_params, \
            'Too few parameters are chosen or in the model'
    
        #for i, layer in enumerate(self.model.children()):
        #    if i in self.layers:
        #        assert hasattr(layer, 'weight'), \
        #            'One of the chosen layers has no weights'
        
        if self.index_permutation is not None:
            assert len(set(self.index_permutation)) == len(self.index_permutation), \
                'Looks like index permutation contains duplicate values'
            
            # Check that all layers in index permutation are layers with weights
            #index_layers = set(map(lambda coord: coord[0], self.index_permutation))
            #layers_without_weights = []
            #for i, layer in enumerate(self.model.children()):
            #    if not hasattr(layer, 'weight'):
            #        layers_without_weights.append(i)
            #assert True not in map(lambda a: a in layers_without_weights, index_layers), \
            #    'One of layers in index permutation refers to layer without weights'
        
        return True
    
    def set_strategy(self, strategy):
        # 'first', 'manual'
        self.strategy = strategy
    
    def set_strategy_index_permutation(self):
        raise NotImplementedError(
            'Function get_strategy_weights is not implemented to BaseEmbedder. ' + 
            'Consider using one of the subclasses')
    
    def payload_bits(self):
        raise NotImplementedError(
            'Function payload_bits is not implemented to BaseEmbedder. ' + 
            'Consider using one of the subclasses')

    def updated_param(self, param, new_bits):
        raise NotImplementedError(
            'Function update_param is not implemented to BaseEmbedder. ' + 
            'Consider using one of the subclasses')
    
    def _embed_method_info(self, inplace=False):
        raise NotImplementedError(
            'Function _embed_method_info is not implemented to BaseEmbedder. ' + 
            'Consider using one of the subclasses')
    
    def _embed_payload(self, inplace=False):
        
        self.check_embeddability()
        
        # INPLACE
        if inplace:
            model = self.model
        else:
            model = self.shadow_model
            
        # EMBEDDING
        with torch.no_grad():
        
            next_bits = self.payload_bits()
        
            for index in self.index_permutation:
                
                index_layer = index[0]
                index_coord = tuple(index[1:])
            
                for i, (name, layer) in enumerate(model.named_parameters()):
                
                    if i == index_layer:
                        new_bits = next(next_bits)
                        new_value = self.updated_param(
                                    layer[index_coord], new_bits)
                        layer[index_coord] = new_value
    
    
    def embed_payload(self, inplace=False):
        self.check_embeddability()
        if not inplace:
            self.shadow_model = copy.deepcopy(self.model)
        self._embed_payload(inplace)
    
    
    def get_model_info(self):
        title_str_begin = "======================= EMBEDDING INIT SUMMARY ========================="

        layer_str = "Layer Info:\n"
        layer_str += (
            "------------------------------------------------------------------------"
            + "\n"
        )
        line_new = "{:>5}  {:>15}  {:>10} {:>20} {:>13}".format(
            "Layer #", "Layer type", "Dimensions", "Weight Shape", "Params #"
        )
        layer_str += line_new + "\n"
        layer_str += (
            "------------------------------------------------------------------------"
            + "\n"
        )
        total_params = 0
        for index, (name, layer) in enumerate(self.model.named_parameters()):

            params = reduce(lambda a, b: a*b, list(layer.shape))
            total_params += params
            dimensions = str(len(layer.shape))
            shape = str(list(layer.shape))
            
            line_new = "{:>5}  {:>15}  {:>10} {:>20} {:>13}".format(
                str(index),
                name,
                str(dimensions),
                str(shape),
                str(params),
            )
            layer_str += line_new + "\n"

        title_str_end = (
            "========================================================================"
        )

        params_str = "Total params: {}\n".format(total_params)
        
        return [title_str_begin,
            layer_str,
            title_str_end,
            params_str]
    
    def print_model_info(self):
        print('\n'.join(self.get_model_info()))




class LsbEmbedder(BaseEmbedder):
    
    def __init__(self, model, bit_length, layers=None, param_length=32):
        super().__init__(model, layers)
        self.param_length = param_length
        self.bit_length = bit_length
    
    def get_num_of_params(self):
        num_of_params = (len(self.payload_bytes) + 1) // self.bit_length
        return num_of_params
    
    
    def check_embeddability(self):
        assert self.bit_length is not None, \
            'You must specify lsb bit_length'
    
        super().check_embeddability()
        
        necessary_params = self.get_num_of_params()
        if self.index_permutation is not None:
            assert len(self.index_permutation) * self.bit_length >= necessary_params, \
                'Too few parameters are in index permutation to fit the payload'
        
        return True
    
    def set_strategy_index_permutation(self):
        
        self.check_embeddability()
        
        def inc(coord, shape):
            if coord[-1] + 1 >= shape[-1]:
                if len(coord) == 1:
                    return [0] * len(shape)
                return inc(coord[:-1], shape[:-1]) + [0]
            return coord[:-1] + [coord[-1] + 1]
        
        def is_zero(coord):
            for i in coord:
                if i != 0:
                    return False
            return True
    
        param_num = self.get_num_of_params()
        if self.strategy == 'first':
            self.index_permutation = []
            with torch.no_grad():
                for i, (name, layer) in enumerate(self.model.named_parameters()):
                    if i in self.layers:
                        coord = [0] * len(layer.shape)
                        while param_num > 0:
                            self.index_permutation.append((i, *coord))
                            coord = inc(coord, layer.shape)
                            param_num -= 1
                            if is_zero(coord):
                                break
                        if param_num <= 0:
                            break
        
        self.check_embeddability()
    
    def payload_bits(self):
        for i in range(0, len(self.payload_bytes), self.bit_length):
            yield self.payload_bytes[i:
                min(i+self.bit_length, len(self.payload_bytes))]
            
    
    def updated_param(self, param, new_bits):
        bit_length = len(new_bits)
        bin_param = Encoder.encode_float(param, self.param_length)
        bin_param = bin_param[:-bit_length] + new_bits
        return Encoder.decode_float(bin_param, self.param_length)
    
    
    def extract(self):
        #self.bit_length
        #self.payload_hash     # TODO
        self.last_bit_length = self.bit_length
        #self.index_permutation
        
        if self.shadow_model is not None:
            model = self.shadow_model
        else:
            model = self.model
        
        payload = ''
        for coord_i, coord in enumerate(self.index_permutation):
            coord_layer = coord[0]
            coord_param = tuple(coord[1:])
            for i, (name, layer) in enumerate(model.named_parameters()):
                if coord_layer == i:
                    bin_val = Encoder.encode_float(layer[coord_param], self.param_length)
                    if coord_i == len(self.index_permutation)-1:
                        payload_bin = bin_val[-self.last_bit_length:]
                    else:
                        payload_bin = bin_val[-self.bit_length:]
                    payload = payload + payload_bin
        self.ext_payload_bytes = payload
        return payload
    
    
    def print_model_info(self):
        [title_str_begin,
        layer_str,
        title_str_end,
        params_str] = self.get_model_info()
        
        method_str = "Method Info:\n"
        method_str += "------------------------------------------------------------------------"
        method_str += "\n"
        method_str += "    - Method: LSB\n"
        method_str += "    - Strategy: first\n"
        method_str += "    - LSB bit length: " + str(self.bit_length)
        method_str += "\n"
        
        if self.layers is not None:
            method_str += "    - Embedding Layers: " + str(self.layers) + "\n"
        
        print(title_str_begin)
        print(method_str)
        print(layer_str)
        print(title_str_end)
        print(params_str)











